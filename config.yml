## AWS
region: 'us-east-1'
profile_name: 'default'
credentials_csv_filepath: 'C:\Users\Jakob\Downloads\jakob-s3-ec2-athena_accessKeys.csv'
batch_role: 'arn:aws:iam::425352751544:role/cc-download' # role to use for the batch job, needs access to S3, Athena, AmazonElasticContainerRegistryPublic

## input paths
url_list_path: 'swiss-survey.csv' # path to url list
url_keywords_path: 'url_keywords.csv' # path to file with url keywords (downloads subpages with these keywords in the url), set to null if not needed
keywords_path: 'keywords.csv' # path to fill with keywords (downloads html paragraphs with any of these keywords), set to null if not needed

## output paths
output_bucket: 'cc-download-swiss-survey' # s3 bucket to store the results, use a new bucket for each experiment

## download settings
crawls: # crawls to include (can be multiple), for list see https://commoncrawl.org/the-data/get-started/
  - 'CC-MAIN-2020-45'
  - 'CC-MAIN-2020-50'
  - 'CC-MAIN-2021-04'
  - 'CC-MAIN-2021-10'
  - 'CC-MAIN-2021-17'
  - 'CC-MAIN-2021-21'
n_subpages_per_domain: 30 # maximum number of subpages to download per domain (downloads the n_subpages_per_domain subpages with the shortest url). Set to 0 if only subpages containing a keyword from the url keyword list should be downloaded. Set to null if all subpages should be downloaded.
limit_pages_url_keywords: 200 # maximum number of subpages containing a keyword from the url keyword list to download per domain
filter_lang: null # 'eng' # filter for language (iso 639-3 code) of the subpages to download, set to null if not needed
one_snapshot_per_url: True # If one_snapshot_per_url=True and a URL is present in multiple crawls, keeps only the version with the biggest payload (if tied, use the first crawl)

## batch settings
image_name: 'public.ecr.aws/cc-download/cc-download:latest' # docker image to use for the batch job
batch_size: 2000 # number of subpages to download per batch
retry_attempts: 3 # number of times to retry a failed download batch
attempt_duration: 4000 # duration in seconds to wait before retrying to download a batch
vcpus: 0.25 # number of vcpus per container, possible values: 0.25, 0.5, 1, 2, 4, 8, 16, 32, 64, 128, 256
memory: 512 # memory in MB per container, possible values: 512, 1024, 2048, 4096, 8192

## debug settings
debug: False   # set to True to enable debug mode (only downloads 10 subpages per domain), keeps large tables, compute environment, job queue
keep_urls_merged_cc: True # set to True if re-running with the same crawls and input urls, to avoid expensive merging of the input urls with the common crawl index
